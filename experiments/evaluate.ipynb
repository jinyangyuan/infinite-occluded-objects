{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import numpy as np\n",
    "import os\n",
    "from evaluation import load_dataset, load_results, compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores_sub(folder_data, name_data, folder_result, filename_base, num_tests, is_ordered):\n",
    "    images, labels = load_dataset(folder_data, name_data)\n",
    "    scores_list = {key: [] for key in ['LL_M', 'LL_S', 'AMI', 'ARI', 'RMSE', 'OCA']}\n",
    "    if is_ordered:\n",
    "        scores_list['OOA'] = []\n",
    "    for model_id in range(num_tests):\n",
    "        results = load_results(folder_result, filename_base.format(model_id))\n",
    "        scores = compute_scores(results, labels, is_ordered)\n",
    "        for key in scores_list:\n",
    "            if key in ['LL_M', 'LL_S']:\n",
    "                scores_list[key].append(scores[key])\n",
    "            elif key in ['RMSE']:\n",
    "                scores_list[key].append(scores[key] * 1e1)\n",
    "            else:\n",
    "                scores_list[key].append(scores[key] * 1e2)\n",
    "    scores_mean = {key: np.mean(val) for key, val in scores_list.items()}\n",
    "    scores_std = {key: np.std(val) for key, val in scores_list.items()}\n",
    "    print('LL_M:{:7.5g} {:.0e}'.format(scores_mean['LL_M'], scores_std['LL_M']), end=' ' * 3)\n",
    "    print('LL_S:{:7.5g} {:.0e}'.format(scores_mean['LL_S'], scores_std['LL_S']))\n",
    "    print('AMI(%):{:5.3g} {:.0e}'.format(scores_mean['AMI'], scores_std['AMI']), end=' ' * 3)\n",
    "    print('ARI(%):{:5.3g} {:.0e}'.format(scores_mean['ARI'], scores_std['ARI']), end=' ' * 3)\n",
    "    print('RMSE(e-1):{:4.2g} {:.0e}'.format(scores_mean['RMSE'], scores_std['RMSE']), end=' ' * 3)\n",
    "    print('OCA(%):{:5.3g} {:.0e}'.format(scores_mean['OCA'], scores_std['OCA']), end=' ' * 3)\n",
    "    if is_ordered:\n",
    "        print('OOA(%):{:5.3g} {:.0e}'.format(scores_mean['OOA'], scores_std['OOA']), end='')\n",
    "    print()\n",
    "    return\n",
    "\n",
    "def print_scores(mode, num_objects='2_3', filename_base='result_{}.h5', num_tests=5):\n",
    "    for name_data_base in name_data_base_list:\n",
    "        is_ordered = False if mode == 'sep' else name_data_base in ['rgb_1', 'rgb_3']\n",
    "        for name_data_sub in name_data_sub_list:\n",
    "            print('{}_{}'.format(name_data_base, name_data_sub))\n",
    "            folder_data = os.path.join(folder_data_base, '{}_{}'.format(mode, name_data_base))\n",
    "            name_data = '{}_{}'.format(name_data_sub, num_objects)\n",
    "            folder_result = '{}_{}_{}'.format(mode, name_data_base, name_data_sub)\n",
    "            print_scores_sub(folder_data, name_data, folder_result, filename_base, num_tests, is_ordered)\n",
    "    return\n",
    "\n",
    "folder_data_base = '../data'\n",
    "name_data_base_list = ['gray', 'rgb_1', 'rgb_2', 'rgb_3', 'rgb_4']\n",
    "name_data_sub_list = ['shapes', 'mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 2 or 3 objects (without occlusion)\n",
    "print_scores('sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 2 or 3 objects (with occlusion)\n",
    "print_scores('occ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 4 objects (without occlusion, K = 4)\n",
    "print_scores('sep', num_objects='4', filename_base='general_4_result_{}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 4 objects (without occlusion, K = 10)\n",
    "print_scores('sep', num_objects='4', filename_base='general_10_result_{}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 4 objects (with occlusion, K = 4)\n",
    "print_scores('occ', num_objects='4', filename_base='general_4_result_{}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on images containing 4 objects (with occlusion, K = 10)\n",
    "print_scores('occ', num_objects='4', filename_base='general_10_result_{}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
